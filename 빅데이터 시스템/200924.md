

# Linear regression

여러 데이터를 분석해서 오차가 가장 적은 하나의 직선을 구하는 것.
빅데이터, 인공지능에서 중요한 수치로 사용

### 개념 설명

![](https://images.velog.io/images/aszxvcb/post/eea2750d-b90e-4751-b730-4cbe0682ee21/image.png)

![](https://images.velog.io/images/aszxvcb/post/fa06f917-93ac-488e-8702-39611699145a/image.png)

![](https://images.velog.io/images/aszxvcb/post/49464bab-512e-4ad7-b5ef-80f6dc735884/image.png)

![](https://images.velog.io/images/aszxvcb/post/d1755355-54ae-4816-b7de-54e7ca724aa9/image.png)

: 여러 직선을 그릴 수 있지만, 오차가 가장 적은 하나의 직선을 선택한다.

![](https://images.velog.io/images/aszxvcb/post/a6d20b54-8e38-49a8-9105-5e29358ff71a/image.png)

![](https://images.velog.io/images/aszxvcb/post/af23db24-eb85-4bc4-b63f-1826bd605540/image.png)

![](https://images.velog.io/images/aszxvcb/post/94b238cb-36a0-4239-b21a-2e1eeadab817/image.png)

: m = 개수, y = 실제값, h = 예측값
: 오차의 제곱의 합을 구한 후, 평균을 구하면 cost가 된다.

![](https://images.velog.io/images/aszxvcb/post/8fa9a446-dd36-4738-8bc1-a503c3bec076/image.png)

![](https://images.velog.io/images/aszxvcb/post/3cc8208b-8ade-40c0-82d6-f35bdd2e4c80/image.png)

: 기울기의 반대 방향으로 조절하면서 적절한 값을 찾는다.

![](https://images.velog.io/images/aszxvcb/post/ffdfdf56-afe9-417c-a386-47d15e7025ff/image.png)

![](https://images.velog.io/images/aszxvcb/post/ba06da01-9f6b-4bfe-8d96-4e55e70e9bb8/image.png)

![](https://images.velog.io/images/aszxvcb/post/92c4c945-fdad-4ddb-a04a-d28a07eadae7/image.png)

![](https://images.velog.io/images/aszxvcb/post/be95799b-57f9-48de-b4d8-0bb2610c8947/image.png)

![](https://images.velog.io/images/aszxvcb/post/5bd390c3-69bb-4f0a-98c1-89b5f9913d4f/image.png)

![](https://images.velog.io/images/aszxvcb/post/f969fa00-2574-4c78-ae2b-19fa6a13d1e8/image.png)



### 코드


```python
import numpy as np
import matplotlib.pyplot as plt
N = 500
X = 2 * np.random.rand(500,1)  #uniform distribution
y = 4 + 3 * X + np.random.randn(500, 1)  #standard normal distribution
```


```python
print(X.shape)
print(y.shape)
print(max(X))
print(min(X))
print(max(y))
print(min(y))
```

    (500, 1)
    (500, 1)
    [1.99953881]
    [6.57142941e-05]
    [11.70306469]
    [2.02527574]



```python
plt.scatter(X,y)

# 500개의 데이터들이 표에 나타남
```




    <matplotlib.collections.PathCollection at 0x7fee748681d0>



![](https://images.velog.io/images/aszxvcb/post/bab932cf-3483-450f-ae92-763de428d669/image.png)


```python
# 보기 편하게 설정
plt.figure(figsize = (10,10))
plt.scatter(X,y)
plt.title("Linear Regreesion Data")
plt.xlabel("x")
plt.ylabel("y")
plt.show()
```


![](https://images.velog.io/images/aszxvcb/post/2252c33d-7b9b-4249-ad8a-e6b6197ac8b6/image.png)



```python
theta1 = 3
theta0 = 4
theta = np.array([[theta1, theta0]]).T
print(theta)
```

    [[3]
     [4]]



```python
oneColumns = np.full((N,1),1)
print(oneColumns.shape)
X1 = np.column_stack((X, oneColumns))
print(X1.shape)
print(X1[:10,:])
```

    (500, 1)
    (500, 2)
    [[1.85797371 1.        ]
     [1.29154213 1.        ]
     [0.557585   1.        ]
     [1.06541859 1.        ]
     [1.02518478 1.        ]
     [1.9769918  1.        ]
     [0.02221317 1.        ]
     [1.50834988 1.        ]
     [1.97880985 1.        ]
     [1.55130226 1.        ]]


**predictions = X1 * Theta**

**X1.shape = (500,2), theta.shape = (2,1) 이므로, predictions 는 (500,1)**

**500개의 X에 대한 prediction**ㄴ


```python
predictions = np.matmul(X1, theta) # 행렬 곱 실행
```


```python
predictions.shape
```




    (500, 1)




```python
plt.figure(figsize=(10,10))
plt.scatter(X,y)
plt.plot(X, predictions, color = "red")
plt.title("Linear Regression Data")
plt.xlabel("x")
plt.ylabel("y")
plt.show()
```


![](https://images.velog.io/images/aszxvcb/post/84e0dbdd-0803-4bcf-a9bc-06c0b1a613d2/image.png)


#### cost 계산
![](https://images.velog.io/images/aszxvcb/post/6932a3cc-0c19-4ec1-81a3-312fa475c34e/image.png)


```python
# Cost 계산
a = np.array([ [1],[2],[3] ])
b = np.array([ [2],[4],[1] ])
c = a - b
print(a)
print(b)
print(c)
d = np.square(c)
print(d)
print(np.sum(d))
```

    [[1]
     [2]
     [3]]
    [[2]
     [4]
     [1]]
    [[-1]
     [-2]
     [ 2]]
    [[1]
     [4]
     [4]]
    9



```python
print(X1.shape)
print(X1.shape[0])
print(y.shape)
print(predictions.shape)

```

    (500, 2)
    500
    (500, 1)
    (500, 1)



```python
def cost(X, y, theta):
    m = X.shape[0]
    predictions = np.matmul(X, theta)
    diff = predictions - y
    cost = (1/2*m)*np.sum(np.square(diff))
    return cost
```


```python
print(cost(X1, y, theta))
```

    129436.48714975052


#### theta update


```python
x = np.array([ [10,1], [12,1], [20,1] ])
error = np.array([ [-1], [1],[2] ])
print(x)
print(error)
print(x.T)
delta = np.matmul(x.T, error)
print(delta)
```

    [[10  1]
     [12  1]
     [20  1]]
    [[-1]
     [ 1]
     [ 2]]
    [[10 12 20]
     [ 1  1  1]]
    [[42]
     [ 2]]



```python
a = np.array([ [10],[20] ])
print(a)
b = np.array([ [42],[2] ])
c = a - 0.4 * b
print(c)
```

    [[10]
     [20]]
    [[-6.8]
     [19.2]]



![](https://images.velog.io/images/aszxvcb/post/0f3b9326-447a-4f07-b71d-e7ce4c00befc/image.png)

delta를 구해서 theta를 수정한다.
delta : x.T * error


![](https://images.velog.io/images/aszxvcb/post/a228e5a7-aa73-4851-a1fe-dea3ca5734a7/image.png)


```python
def linear_regression(X, theta, y, alpha):
    m = X.shape[0] # 데이터의 갯수
    num_iterations = 100
    costs = []
    thetas = [theta]
    for i in range(num_iterations):
        predictions = np.matmul(X1, theta)
        errors = predictions - y
        delta = np.matmul(X.T, errors)
        
        theta = theta - (alpha/m) * delta
        cost1 = cost(X,y,theta)
        costs.append(cost1)
        thetas.append(theta)
    return costs, thetas
```


```python
theta1 = -5
theta0 = 5
theta = np.array([[theta1, theta0]]).T
costs, thetas = linear_regression(X1, theta, y, 0.5)
```


```python
print(len(costs), len(thetas) )
```

    100 101



```python
plt.plot(costs) # cost가 점점 줄어드는 것을 볼 수 있다.
```




    [<matplotlib.lines.Line2D at 0x7fee77bf5470>]




![](https://images.velog.io/images/aszxvcb/post/12310cc6-6d60-4905-8509-db48f44c49b0/image.png)


#### Initial and final regression


```python
initialTheta = thetas[0] # 첫 세타 값
print(initialTheta)
finalTheta = thetas[-1] # 이터레이션이 충분히 이뤄진 개선된 세타
print(finalTheta)
prediction1 = np.matmul(X1, initialTheta)
prediction2 = np.matmul(X1, finalTheta)
```

    [[-5]
     [ 5]]
    [[3.0549479 ]
     [3.96520498]]



```python
plt.figure(figsize=(10,10))
plt.scatter(X, y)
plt.plot(X, prediction1, color="red")
plt.plot(X, prediction2, color="blue")
plt.title("Linear Regression Data")
plt.xlabel("x")
plt.ylabel("y")
plt.show()
```


![](https://images.velog.io/images/aszxvcb/post/4cf5d827-d398-438b-b3a5-a9724b83a9c7/image.png)

